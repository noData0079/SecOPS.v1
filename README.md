# SecOPS.v1

A curated reference for SecOPS.v1 resources.

## LLM Systems & Serving
- [llama.cpp](https://github.com/ggerganov/llama.cpp): Lightweight C/C++ inference engine for running LLaMA-family models efficiently on local CPUs/GPUs.
- [ollama](https://github.com/ollama/ollama): Local-first model runner with simple APIs and model packaging for serving LLMs on desktops or servers.
- [open-webui](https://github.com/open-webui/open-webui): Self-hosted web interface that connects to local or remote models, providing chat and management features for LLM deployments.
- [vLLM](https://github.com/vllm-project/vllm): High-throughput LLM serving engine that uses PagedAttention for efficient inference and streaming.
