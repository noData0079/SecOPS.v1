# Project A (vLLM-style model server) dependencies
# Align transformer version with the backend to avoid resolver conflicts.
vllm==0.5.4.post1
transformers==4.44.2
# CUDA-enabled torch is expected for GPU inference; pin a stable CPU fallback for local tests.
torch==2.3.1
numpy==1.26.4
